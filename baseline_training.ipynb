{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0473d80-25e2-4de5-82c8-2f32e39241aa",
   "metadata": {},
   "source": [
    "# All libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b87f4d-4ecb-42c6-a39f-9ab04d11a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from itertools import product\n",
    "import warnings\n",
    "from pandas.errors import PerformanceWarning\n",
    "\n",
    "# Suppress fragmentation warnings from pandas\n",
    "warnings.filterwarnings(\"ignore\", category=PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff092e2e",
   "metadata": {},
   "source": [
    "Below is the training code for the Baseline model, which requires a relatively long training time. The training results are saved as a CSV file in the working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7083e04c-5534-4616-a404-ea9128504b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "real_volatility = pd.read_csv('real_volatility.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18cfe6fa-2d82-473b-8e13-ddb5b9e12abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate All metrics needed for performance\n",
    "def metric(prediction, actual):\n",
    "    prediction, actual = np.array(prediction), np.array(actual)\n",
    "    eps = 1e-8\n",
    "    Prediction = np.maximum(prediction, eps)\n",
    "    Actual = np.maximum(actual, eps)\n",
    "    RMSE = np.sqrt(np.mean((Prediction - Actual) ** 2))\n",
    "    QLIKE = np.mean(Actual / Prediction - np.log(Actual / Prediction) - 1)\n",
    "    RMPSE = np.sqrt(np.mean(np.square((actual - prediction) / actual))) * 100\n",
    "    MAPE = np.mean(np.abs((actual - prediction) / actual)) * 100\n",
    "    return RMSE, QLIKE, RMPSE, MAPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beee6495-81b9-46ab-9090-92a3b28edcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print summary for each model\n",
    "def print_summary(RMSE, QLIKE, RMPSE, MAPE, NAME):\n",
    "    print(f'\\nPerformance Summary for {NAME}')\n",
    "    print(f\"Average RMSE: {np.mean(RMSE):.6f}\")\n",
    "    print(f\"Average QLIKE: {np.mean(QLIKE):.6f}\")\n",
    "    print(f\"Average RMPSE: {np.mean(RMPSE):.6f}%\")\n",
    "    print(f\"Average MAPE: {np.mean(MAPE):.6f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce4970-a915-4b2d-a38e-ca0375ab6714",
   "metadata": {},
   "source": [
    "# HAR-RV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a394b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Summary for HAR-RV\n",
      "Average RMSE: 0.001721\n",
      "Average QLIKE: 0.149627\n",
      "Average RMPSE: 60.295211%\n",
      "Average MAPE: 46.612905%\n",
      "\n",
      "Best Hyperparameters for HAR-RV Model:\n",
      "--------------------------------------------------\n",
      "Stock           Daily Lag  Weekly Window   Monthly Window \n",
      "--------------------------------------------------\n",
      "0               1          5               30             \n",
      "1               1          5               30             \n",
      "2               1          5               30             \n",
      "3               1          5               30             \n",
      "4               1          5               30             \n",
      "5               1          5               30             \n",
      "6               1          5               30             \n",
      "7               1          5               30             \n",
      "8               1          5               30             \n",
      "9               1          5               30             \n",
      "10              1          5               30             \n",
      "11              1          5               30             \n",
      "14              1          5               30             \n",
      "15              1          5               30             \n",
      "16              1          5               30             \n",
      "17              1          5               30             \n",
      "19              1          5               30             \n",
      "20              1          5               30             \n",
      "21              1          5               30             \n",
      "22              1          5               30             \n",
      "23              1          5               30             \n",
      "26              1          5               30             \n",
      "27              1          5               30             \n",
      "28              1          5               30             \n",
      "29              1          5               30             \n",
      "30              1          5               30             \n",
      "33              1          7               30             \n",
      "34              1          5               30             \n",
      "35              1          5               30             \n",
      "36              1          5               30             \n",
      "37              1          5               30             \n",
      "39              1          5               30             \n",
      "40              1          5               30             \n",
      "41              1          5               30             \n",
      "42              1          5               30             \n",
      "43              1          5               30             \n",
      "44              1          5               30             \n",
      "47              1          5               30             \n",
      "48              1          5               30             \n",
      "50              1          5               30             \n",
      "51              1          5               30             \n",
      "52              1          5               30             \n",
      "53              1          5               30             \n",
      "55              1          5               30             \n",
      "56              1          5               30             \n",
      "58              1          5               30             \n",
      "59              1          5               30             \n",
      "60              1          5               30             \n",
      "61              1          5               30             \n",
      "62              1          7               30             \n",
      "63              1          5               30             \n",
      "64              1          5               30             \n",
      "66              1          5               30             \n",
      "67              1          5               30             \n",
      "68              1          5               30             \n",
      "69              1          5               30             \n",
      "70              1          5               30             \n",
      "72              1          5               30             \n",
      "73              1          5               30             \n",
      "74              1          5               30             \n",
      "76              1          5               30             \n",
      "77              1          5               30             \n",
      "78              1          5               30             \n",
      "81              1          5               30             \n",
      "82              1          5               30             \n",
      "83              1          5               30             \n",
      "84              1          5               30             \n",
      "85              1          5               30             \n",
      "86              1          5               30             \n",
      "87              1          5               30             \n",
      "88              1          5               30             \n",
      "90              1          5               30             \n",
      "93              1          5               30             \n",
      "94              1          5               30             \n",
      "95              1          5               30             \n",
      "96              1          5               30             \n",
      "97              1          5               30             \n",
      "98              1          5               30             \n",
      "99              1          5               30             \n",
      "101             1          5               30             \n",
      "102             1          5               30             \n",
      "103             1          5               30             \n",
      "104             1          5               30             \n",
      "105             1          5               30             \n",
      "107             1          5               30             \n",
      "108             1          5               30             \n",
      "109             1          5               30             \n",
      "110             1          5               30             \n",
      "111             1          5               30             \n",
      "112             1          5               30             \n",
      "113             1          5               30             \n",
      "114             1          5               30             \n",
      "115             1          5               30             \n",
      "116             1          5               30             \n",
      "118             1          5               30             \n",
      "119             1          5               30             \n",
      "120             1          5               30             \n",
      "122             1          5               30             \n",
      "123             1          5               30             \n",
      "124             1          5               30             \n",
      "125             1          5               30             \n",
      "126             1          5               30             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/czngg_nd6lld29qrwpwnttwr0000gn/T/ipykernel_11369/1607380092.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  har_panel[stock] = pd.Series(final_Prediction, index=valid_index)\n",
      "/var/folders/4l/czngg_nd6lld29qrwpwnttwr0000gn/T/ipykernel_11369/1607380092.py:83: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  har_panel[stock] = pd.Series(final_Prediction, index=valid_index)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# lists to store result\n",
    "har_panel = pd.DataFrame()\n",
    "QLIKE_list = []\n",
    "RMSE_list = []\n",
    "RMPSE_list = []\n",
    "MAPE_list = []\n",
    "# Dictionary to store best parameters for each stock\n",
    "best_params_dict = {}\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "lag_matrix = {\n",
    "    'daily': [1],\n",
    "    'weekly': [5, 7],\n",
    "    'monthly': [22, 30]\n",
    "}\n",
    "lag_matrix = list(product(lag_matrix['daily'], lag_matrix['weekly'], lag_matrix['monthly']))\n",
    "\n",
    "# go through each unique stock\n",
    "for stock in real_volatility.columns:  \n",
    "    stock_series = real_volatility[stock]\n",
    "    log_stock = np.log(stock_series)\n",
    "\n",
    "    # Initialise the best performance tracker\n",
    "    final_MAPE = float('inf')\n",
    "    final_Prediction = None\n",
    "    final_Actual = None\n",
    "    best_params = None  # Track best parameters for this stock\n",
    "\n",
    "    # Grid Search each combination\n",
    "    for daily, weekly, monthly in lag_matrix:\n",
    "        har_data = pd.DataFrame({'volatility': log_stock})      \n",
    "        har_data['rv_day'] = log_stock.shift(daily)\n",
    "        har_data['rv_week'] = log_stock.rolling(window=weekly).mean().shift(1)\n",
    "        har_data['rv_month'] = log_stock.rolling(window=monthly).mean().shift(1)\n",
    "        har_data['trend'] = np.arange(len(log_stock))\n",
    "        har_data = har_data.dropna()\n",
    "\n",
    "        # Train-Validation-Test split\n",
    "        number = len(har_data)\n",
    "        train_size = int(number * 0.8)\n",
    "        validation_size = int(number * 0.1)\n",
    "\n",
    "        har_train = har_data.iloc[:train_size]\n",
    "        har_validation = har_data.iloc[train_size:train_size + validation_size]\n",
    "        har_test = har_data.iloc[train_size + validation_size:]\n",
    "\n",
    "        feature_columns = ['rv_day', 'rv_week', 'rv_month', 'trend']\n",
    "        \n",
    "        # Prepare the dataset \n",
    "        train_X = har_train[feature_columns]\n",
    "        train_y = har_train['volatility']\n",
    "        validation_X = har_validation[feature_columns]\n",
    "        validation_y = har_validation['volatility']\n",
    "\n",
    "        # fit the linear model\n",
    "        har_model = LinearRegression()\n",
    "        har_model.fit(train_X, train_y)\n",
    "\n",
    "        # evaluate on the validation set\n",
    "        validation_prediction = np.exp(har_model.predict(validation_X))\n",
    "        validation_actual = np.exp(validation_y)\n",
    "        validation_RMSE, validation_QLIKE, validation_RMPSE, validation_MAPE = metric(validation_actual, validation_prediction)\n",
    "\n",
    "        # check if this the best lag combination\n",
    "        if validation_MAPE < final_MAPE:\n",
    "            final_MAPE = validation_MAPE\n",
    "            test_features = har_test[feature_columns]\n",
    "            test_target = har_test['volatility']\n",
    "            \n",
    "            valid_index = test_features.dropna().index\n",
    "            test_X = test_features.loc[valid_index]\n",
    "            test_y = test_target.loc[valid_index]\n",
    "\n",
    "            # evaluate on the test set\n",
    "            final_Prediction = np.exp(har_model.predict(test_X))\n",
    "            final_Actual = np.exp(test_y)\n",
    "            \n",
    "            # Store best parameters for this stock\n",
    "            best_params = {'daily': daily, 'weekly': weekly, 'monthly': monthly}\n",
    "\n",
    "    # Store Result\n",
    "    if final_Prediction is not None:\n",
    "        har_panel[stock] = pd.Series(final_Prediction, index=valid_index)\n",
    "\n",
    "        RMSE, QLIKE, RMPSE, MAPE = metric(final_Prediction, final_Actual)\n",
    "    \n",
    "        MAPE_list.append(MAPE)\n",
    "        RMSE_list.append(RMSE)\n",
    "        QLIKE_list.append(QLIKE)\n",
    "        RMPSE_list.append(RMPSE)\n",
    "        \n",
    "        # Store best parameters in the dictionary\n",
    "        best_params_dict[stock] = best_params\n",
    "\n",
    "print_summary(RMSE_list, QLIKE_list, RMPSE_list, MAPE_list, 'HAR-RV')\n",
    "\n",
    "# Print best parameters for each stock\n",
    "print(\"\\nBest Hyperparameters for HAR-RV Model:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Stock':<15} {'Daily Lag':<10} {'Weekly Window':<15} {'Monthly Window':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for stock, params in best_params_dict.items():\n",
    "    print(f\"{stock:<15} {params['daily']:<10} {params['weekly']:<15} {params['monthly']:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3470a8-b5be-4889-b1b3-acdefef64670",
   "metadata": {},
   "source": [
    "# Lag Model Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "638f5ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Summary for Lag Model\n",
      "Average RMSE: 0.001958\n",
      "Average QLIKE: 0.193517\n",
      "Average RMPSE: 106.055586%\n",
      "Average MAPE: 83.635238%\n",
      "\n",
      "Best Lag Parameters for Lag Model:\n",
      "----------------------------------------\n",
      "Stock           Best Lag  \n",
      "----------------------------------------\n",
      "0               2         \n",
      "1               1         \n",
      "2               1         \n",
      "3               2         \n",
      "4               2         \n",
      "5               2         \n",
      "6               2         \n",
      "7               1         \n",
      "8               1         \n",
      "9               1         \n",
      "10              1         \n",
      "11              2         \n",
      "14              2         \n",
      "15              2         \n",
      "16              1         \n",
      "17              1         \n",
      "19              2         \n",
      "20              1         \n",
      "21              2         \n",
      "22              2         \n",
      "23              2         \n",
      "26              2         \n",
      "27              1         \n",
      "28              2         \n",
      "29              1         \n",
      "30              1         \n",
      "33              1         \n",
      "34              2         \n",
      "35              1         \n",
      "36              1         \n",
      "37              1         \n",
      "39              2         \n",
      "40              1         \n",
      "41              2         \n",
      "42              2         \n",
      "43              1         \n",
      "44              1         \n",
      "47              2         \n",
      "48              2         \n",
      "50              1         \n",
      "51              2         \n",
      "52              1         \n",
      "53              2         \n",
      "55              1         \n",
      "56              1         \n",
      "58              2         \n",
      "59              2         \n",
      "60              1         \n",
      "61              1         \n",
      "62              2         \n",
      "63              1         \n",
      "64              2         \n",
      "66              1         \n",
      "67              1         \n",
      "68              2         \n",
      "69              2         \n",
      "70              1         \n",
      "72              1         \n",
      "73              1         \n",
      "74              2         \n",
      "76              1         \n",
      "77              1         \n",
      "78              1         \n",
      "81              1         \n",
      "82              2         \n",
      "83              1         \n",
      "84              2         \n",
      "85              1         \n",
      "86              1         \n",
      "87              1         \n",
      "88              1         \n",
      "90              1         \n",
      "93              1         \n",
      "94              1         \n",
      "95              1         \n",
      "96              1         \n",
      "97              1         \n",
      "98              1         \n",
      "99              2         \n",
      "101             1         \n",
      "102             2         \n",
      "103             2         \n",
      "104             2         \n",
      "105             2         \n",
      "107             1         \n",
      "108             1         \n",
      "109             1         \n",
      "110             1         \n",
      "111             1         \n",
      "112             1         \n",
      "113             2         \n",
      "114             1         \n",
      "115             2         \n",
      "116             1         \n",
      "118             1         \n",
      "119             2         \n",
      "120             1         \n",
      "122             1         \n",
      "123             1         \n",
      "124             1         \n",
      "125             1         \n",
      "126             1         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/czngg_nd6lld29qrwpwnttwr0000gn/T/ipykernel_11369/772777921.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  lag_panel[stock] = pd.Series(final_Prediction, index=final_index)\n",
      "/var/folders/4l/czngg_nd6lld29qrwpwnttwr0000gn/T/ipykernel_11369/772777921.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  lag_panel[stock] = pd.Series(final_Prediction, index=final_index)\n"
     ]
    }
   ],
   "source": [
    "# lists to store the value\n",
    "lag_panel = pd.DataFrame()\n",
    "QLIKE_list = []\n",
    "RMSE_list = []\n",
    "RMPSE_list = []\n",
    "MAPE_list = []\n",
    "# Dictionary to store best lag parameter for each stock\n",
    "best_lag_dict = {}\n",
    "\n",
    "# Hyperparameter tuni\n",
    "# ng\n",
    "Lag = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Go through each stock\n",
    "for stock in real_volatility.columns:  \n",
    "    stock_series = real_volatility[stock]\n",
    "    log_stock = np.log(stock_series)\n",
    "\n",
    "    # Initialise for the best performance tracker\n",
    "    final_MAPE = float('inf')\n",
    "    final_Prediction = None\n",
    "    final_Actual = None\n",
    "    final_index = None\n",
    "    best_lag = None  # Track best lag for this stock\n",
    "\n",
    "    # go through each lag threshold\n",
    "    for lag in Lag:\n",
    "        \n",
    "        # feature set\n",
    "        lag_data = pd.DataFrame({\n",
    "            'volatility': log_stock,\n",
    "            'lag': log_stock.shift(lag),\n",
    "            'trend': np.arange(len(log_stock))})\n",
    "        lag_data = lag_data.dropna()\n",
    "\n",
    "        # Train-Validation-Test split\n",
    "        number = len(lag_data)\n",
    "        train_size = int(number * 0.8)\n",
    "        validation_size = int(number * 0.1)\n",
    "\n",
    "        lag_train = lag_data.iloc[:train_size]\n",
    "        lag_Validation = lag_data.iloc[train_size:train_size + validation_size]\n",
    "        lag_test = lag_data.iloc[train_size + validation_size:]\n",
    "\n",
    "        # Prepare the dataset\n",
    "        feature_columns = ['lag', 'trend']\n",
    "        train_X = lag_train[feature_columns]\n",
    "        train_y = lag_train['volatility']\n",
    "        validation_X = lag_Validation[feature_columns]\n",
    "        validation_y = lag_Validation['volatility']\n",
    "        test_X = lag_test[feature_columns]\n",
    "        test_y = lag_test['volatility']\n",
    "        test_X = test_X.dropna()\n",
    "        test_y = test_y.loc[test_X.index]\n",
    "\n",
    "        # fit the model\n",
    "        lag_model = LinearRegression()\n",
    "        lag_model.fit(train_X, train_y)\n",
    "\n",
    "        # evaluate on the validation set\n",
    "        validation_Prediction = np.exp(lag_model.predict(validation_X))\n",
    "        validation_Actual = np.exp(validation_y)\n",
    "        validation_RMSE, validation_QLIKE, validation_RMPSE, validation_MAPE = metric(validation_Prediction, validation_Actual)\n",
    "\n",
    "        # Check is it the best performance\n",
    "        if validation_MAPE < final_MAPE:\n",
    "            final_MAPE = validation_MAPE\n",
    "            final_Prediction = np.exp(lag_model.predict(test_X))\n",
    "            final_Actual = np.exp(test_y)\n",
    "            final_index = test_X.index\n",
    "            best_lag = lag  # Store the best lag\n",
    "\n",
    "    # store the result\n",
    "    if final_Prediction is not None:\n",
    "        lag_panel[stock] = pd.Series(final_Prediction, index=final_index)\n",
    "        \n",
    "        RMSE, QLIKE, RMPSE, MAPE = metric(final_Prediction, final_Actual)\n",
    "    \n",
    "        MAPE_list.append(MAPE)\n",
    "        RMSE_list.append(RMSE)\n",
    "        QLIKE_list.append(QLIKE)\n",
    "        RMPSE_list.append(RMPSE)\n",
    "        \n",
    "        # Store best lag in the dictionary\n",
    "        best_lag_dict[stock] = best_lag\n",
    "\n",
    "# print the overall result\n",
    "print_summary(RMSE_list, QLIKE_list, RMPSE_list, MAPE_list, 'Lag Model')\n",
    "\n",
    "# Print best lag parameters for each stock\n",
    "print(\"\\nBest Lag Parameters for Lag Model:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Stock':<15} {'Best Lag':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for stock, lag in best_lag_dict.items():\n",
    "    print(f\"{stock:<15} {lag:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94903f8d-7c72-43ef-833b-7acf3fb183f2",
   "metadata": {},
   "source": [
    "# PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5f0fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Summary for PCA Model\n",
      "Average RMSE: 0.001723\n",
      "Average QLIKE: 0.151160\n",
      "Average RMPSE: 59.009949%\n",
      "Average MAPE: 45.501839%\n",
      "\n",
      "Best Variance Thresholds for PCA Model:\n",
      "----------------------------------------\n",
      "Stock           Variance Threshold  \n",
      "----------------------------------------\n",
      "0               0.97                \n",
      "1               0.97                \n",
      "2               0.99                \n",
      "3               0.97                \n",
      "4               0.97                \n",
      "5               0.97                \n",
      "6               0.97                \n",
      "7               0.97                \n",
      "8               0.99                \n",
      "9               0.97                \n",
      "10              0.99                \n",
      "11              0.97                \n",
      "14              0.97                \n",
      "15              0.99                \n",
      "16              0.97                \n",
      "17              0.97                \n",
      "19              0.97                \n",
      "20              0.97                \n",
      "21              0.97                \n",
      "22              0.97                \n",
      "23              0.97                \n",
      "26              0.99                \n",
      "27              0.97                \n",
      "28              0.97                \n",
      "29              0.97                \n",
      "30              0.97                \n",
      "33              0.97                \n",
      "34              0.97                \n",
      "35              0.97                \n",
      "36              0.97                \n",
      "37              0.99                \n",
      "39              0.99                \n",
      "40              0.97                \n",
      "41              0.97                \n",
      "42              0.97                \n",
      "43              0.99                \n",
      "44              0.97                \n",
      "47              0.97                \n",
      "48              0.97                \n",
      "50              0.97                \n",
      "51              0.97                \n",
      "52              0.97                \n",
      "53              0.97                \n",
      "55              0.97                \n",
      "56              0.97                \n",
      "58              0.99                \n",
      "59              0.97                \n",
      "60              0.97                \n",
      "61              0.97                \n",
      "62              0.97                \n",
      "63              0.97                \n",
      "64              0.97                \n",
      "66              0.97                \n",
      "67              0.97                \n",
      "68              0.99                \n",
      "69              0.97                \n",
      "70              0.97                \n",
      "72              0.97                \n",
      "73              0.97                \n",
      "74              0.97                \n",
      "76              0.97                \n",
      "77              0.97                \n",
      "78              0.97                \n",
      "81              0.99                \n",
      "82              0.97                \n",
      "83              0.97                \n",
      "84              0.97                \n",
      "85              0.97                \n",
      "86              0.97                \n",
      "87              0.97                \n",
      "88              0.97                \n",
      "90              0.97                \n",
      "93              0.97                \n",
      "94              0.97                \n",
      "95              0.97                \n",
      "96              0.97                \n",
      "97              0.99                \n",
      "98              0.97                \n",
      "99              0.97                \n",
      "101             0.97                \n",
      "102             0.97                \n",
      "103             0.99                \n",
      "104             0.97                \n",
      "105             0.97                \n",
      "107             0.97                \n",
      "108             0.97                \n",
      "109             0.97                \n",
      "110             0.99                \n",
      "111             0.97                \n",
      "112             0.97                \n",
      "113             0.97                \n",
      "114             0.97                \n",
      "115             0.97                \n",
      "116             0.97                \n",
      "118             0.97                \n",
      "119             0.99                \n",
      "120             0.97                \n",
      "122             0.97                \n",
      "123             0.99                \n",
      "124             0.97                \n",
      "125             0.97                \n",
      "126             0.97                \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/czngg_nd6lld29qrwpwnttwr0000gn/T/ipykernel_11369/583285730.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pca_linear_panel[stock] = pd.Series(final_Prediction, index=pca_test.index)\n",
      "/var/folders/4l/czngg_nd6lld29qrwpwnttwr0000gn/T/ipykernel_11369/583285730.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pca_linear_panel[stock] = pd.Series(final_Prediction, index=pca_test.index)\n"
     ]
    }
   ],
   "source": [
    "# List to store value\n",
    "pca_linear_panel = pd.DataFrame()\n",
    "QLIKE_list = []\n",
    "RMSE_list = []\n",
    "RMPSE_list = []\n",
    "MAPE_list = []\n",
    "# Dictionary to store best variance threshold for each stock\n",
    "best_variance_dict = {}\n",
    "\n",
    "# Hyperparameters tuning\n",
    "variance_threshold = [0.9, 0.93, 0.95, 0.97, 0.99]\n",
    "\n",
    "# Go through each individual stock\n",
    "for stock in real_volatility.columns:  \n",
    "    stock_series = real_volatility[stock]\n",
    "    log_stock = np.log(stock_series)\n",
    "\n",
    "    # feature set\n",
    "    pca_data = pd.DataFrame({\n",
    "        'volatility': log_stock,\n",
    "        'trend': np.arange(len(log_stock)),\n",
    "        'lag1': log_stock.shift(1),\n",
    "        'lag2': log_stock.shift(2),\n",
    "        'lag3': log_stock.shift(3),\n",
    "        'moving_avg_3': log_stock.rolling(window=3).mean().shift(1),\n",
    "        'moving_avg_5': log_stock.rolling(window=5).mean().shift(1),\n",
    "        'moving_avg_10': log_stock.rolling(window=10).mean().shift(1),\n",
    "        'std_5': log_stock.rolling(window=5).std().shift(1),\n",
    "        'std_10': log_stock.rolling(window=10).std().shift(1)})\n",
    "\n",
    "    pca_data = pca_data.dropna()\n",
    "\n",
    "    # Train-Validation-Test split\n",
    "    number = len(pca_data)\n",
    "    train_size = int(number * 0.8)\n",
    "    validation_size = int(number * 0.1)\n",
    "\n",
    "    pca_train = pca_data.iloc[:train_size]\n",
    "    pca_validation = pca_data.iloc[train_size:train_size + validation_size]\n",
    "    pca_test = pca_data.iloc[train_size + validation_size:]\n",
    "\n",
    "    # Prepare the dataset \n",
    "    feature_columns = ['trend', 'lag1', 'lag2', 'lag3', 'moving_avg_3', 'moving_avg_5', 'moving_avg_10', 'std_5', 'std_10']\n",
    "    \n",
    "    train_X = pca_train[feature_columns]\n",
    "    train_y = pca_train['volatility']\n",
    "    validation_X = pca_validation[feature_columns]\n",
    "    validation_y = pca_validation['volatility']\n",
    "    test_X = pca_test[feature_columns]\n",
    "    test_y = pca_test['volatility']\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    train_X_scaled = scaler.fit_transform(train_X)\n",
    "    validation_X_scaled = scaler.transform(validation_X)\n",
    "    test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "    # Initialise for the best performance tracker\n",
    "    final_MAPE = float('inf')\n",
    "    final_Prediction = None\n",
    "    final_Actual = None\n",
    "    final_variance = None\n",
    "\n",
    "    # Go through each variance threshold\n",
    "    for variance in variance_threshold:  \n",
    "        pca = PCA(n_components=variance)     \n",
    "        train_pca = pca.fit_transform(train_X_scaled)\n",
    "        validation_pca = pca.transform(validation_X_scaled)\n",
    "\n",
    "        pca_linear_model = LinearRegression()\n",
    "        pca_linear_model.fit(train_pca, train_y)\n",
    "\n",
    "        validation_prediction = np.exp(pca_linear_model.predict(validation_pca))\n",
    "        validation_actual = np.exp(validation_y)\n",
    "        validation_RMSE, validation_QLIKE, validation_RMPSE, validation_MAPE = metric(validation_actual, validation_prediction)\n",
    "\n",
    "        # check if this is the best variance \n",
    "        if validation_MAPE < final_MAPE:\n",
    "            final_MAPE = validation_MAPE\n",
    "            final_variance = variance\n",
    "            final_model = pca_linear_model\n",
    "            final_pca = pca\n",
    "        \n",
    "    # evaluate on test set    \n",
    "    test_pca = final_pca.transform(test_X_scaled)\n",
    "    test_prediction = final_model.predict(test_pca)\n",
    "    final_Prediction = np.exp(test_prediction)\n",
    "    final_Actual = np.exp(test_y)\n",
    "    pca_linear_panel[stock] = pd.Series(final_Prediction, index=pca_test.index)\n",
    "\n",
    "    RMSE, QLIKE, RMPSE, MAPE = metric(final_Prediction, final_Actual)\n",
    "    \n",
    "    MAPE_list.append(MAPE)\n",
    "    RMSE_list.append(RMSE)\n",
    "    QLIKE_list.append(QLIKE)\n",
    "    RMPSE_list.append(RMPSE)\n",
    "    \n",
    "    # Store best variance in the dictionary\n",
    "    best_variance_dict[stock] = final_variance\n",
    "\n",
    "# print final evaluation \n",
    "print_summary(RMSE_list, QLIKE_list, RMPSE_list, MAPE_list, 'PCA Model')\n",
    "\n",
    "# Print best variance parameters for each stock\n",
    "print(\"\\nBest Variance Thresholds for PCA Model:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Stock':<15} {'Variance Threshold':<20}\")\n",
    "print(\"-\" * 40)\n",
    "for stock, variance in best_variance_dict.items():\n",
    "    print(f\"{stock:<15} {variance:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8836e57",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_panel = pd.DataFrame()\n",
    "QLIKE_list = []\n",
    "RMSE_list = []\n",
    "RMPSE_list = []\n",
    "MAPE_list = []\n",
    "# Dictionary to store best n_estimators for each stock\n",
    "best_estimators_dict = {}\n",
    "\n",
    "n_estimators = [200, 300, 500]\n",
    "\n",
    "for stock in real_volatility.columns:  \n",
    "    stock_series = real_volatility[stock]\n",
    "    log_stock = np.log(stock_series)\n",
    "\n",
    "    rf_data = pd.DataFrame({\n",
    "        'volatility': log_stock,\n",
    "        'trend': np.arange(len(log_stock)),\n",
    "        'lag1': log_stock.shift(1),\n",
    "        'lag2': log_stock.shift(2),\n",
    "        'lag3': log_stock.shift(3),\n",
    "        'moving_avg_3': log_stock.rolling(window=3).mean().shift(1),\n",
    "        'moving_avg_5': log_stock.rolling(window=5).mean().shift(1),\n",
    "        'moving_avg_10': log_stock.rolling(window=10).mean().shift(1),\n",
    "        'std_5': log_stock.rolling(window=5).std().shift(1),\n",
    "        'std_10': log_stock.rolling(window=10).std().shift(1),\n",
    "        'max_5': log_stock.rolling(window=5).max().shift(1),\n",
    "        'min_5': log_stock.rolling(window=5).min().shift(1)})\n",
    "\n",
    "    rf_data = rf_data.dropna()\n",
    "\n",
    "    number = len(rf_data)\n",
    "    train_size = int(number * 0.8)\n",
    "    validation_size = int(number * 0.1)\n",
    "\n",
    "    rf_train = rf_data.iloc[:train_size]\n",
    "    rf_validation = rf_data.iloc[train_size:train_size + validation_size]\n",
    "    rf_test = rf_data.iloc[train_size + validation_size:]\n",
    "\n",
    "    feature_columns = ['trend', 'lag1', 'lag2', 'lag3', 'moving_avg_3', 'moving_avg_5', 'moving_avg_10', 'std_5', 'std_10', 'max_5', 'min_5']\n",
    "    \n",
    "    train_X = rf_train[feature_columns]\n",
    "    train_y = rf_train['volatility']\n",
    "    validation_X = rf_validation[feature_columns]\n",
    "    validation_y = rf_validation['volatility']\n",
    "    test_X = rf_test[feature_columns]\n",
    "    test_y = rf_test['volatility']\n",
    "\n",
    "    final_MAPE = float('inf')\n",
    "    final_Prediction = None\n",
    "    final_Actual = None\n",
    "    best_n = None  # Track best n_estimators\n",
    "\n",
    "    for n in n_estimators:\n",
    "        rf_model = RandomForestRegressor(n_estimators=n, random_state=42, max_depth=10)\n",
    "        rf_model.fit(train_X, train_y)\n",
    "        validation_Prediction = np.exp(rf_model.predict(validation_X))\n",
    "        validation_Actual = np.exp(validation_y)\n",
    "        validation_RMSE, validation_QLIKE, validation_RMPSE, validation_MAPE = metric(validation_Actual, validation_Prediction)\n",
    "\n",
    "        if validation_MAPE < final_MAPE:\n",
    "            final_MAPE = validation_MAPE\n",
    "            final_Prediction = np.exp(rf_model.predict(test_X))\n",
    "            final_Actual = np.exp(test_y)\n",
    "            best_n = n  # Store best n_estimators\n",
    "\n",
    "    rf_panel[stock] = pd.Series(final_Prediction, index=rf_test.index)\n",
    "\n",
    "    RMSE, QLIKE, RMPSE, MAPE = metric(final_Prediction, final_Actual)\n",
    "    MAPE_list.append(MAPE)\n",
    "    RMSE_list.append(RMSE)\n",
    "    QLIKE_list.append(QLIKE)\n",
    "    RMPSE_list.append(RMPSE)\n",
    "    \n",
    "    # Store best parameter in the dictionary\n",
    "    best_estimators_dict[stock] = best_n\n",
    "\n",
    "print_summary(RMSE_list, QLIKE_list, RMPSE_list, MAPE_list, 'Random Forest')\n",
    "\n",
    "# Print best parameters for each stock\n",
    "print(\"\\nBest n_estimators for Random Forest Model:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Stock':<15} {'n_estimators':<15}\")\n",
    "print(\"-\" * 40)\n",
    "for stock, n_est in best_estimators_dict.items():\n",
    "    print(f\"{stock:<15} {n_est:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85306b52",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161608af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_panel = pd.DataFrame()\n",
    "QLIKE_list = []\n",
    "RMSE_list = []\n",
    "RMPSE_list = []\n",
    "MAPE_list = []\n",
    "# Dictionary to store best learning rate for each stock\n",
    "best_lr_dict = {}\n",
    "\n",
    "learning_rate = [0.01, 0.05, 0.1]\n",
    "\n",
    "for stock in real_volatility.columns:  \n",
    "    stock_series = real_volatility[stock]\n",
    "    log_stock = np.log(stock_series)\n",
    "\n",
    "    gb_data = pd.DataFrame({\n",
    "        'volatility': log_stock,\n",
    "        'trend': np.arange(len(log_stock)),\n",
    "        'lag1': log_stock.shift(1),\n",
    "        'lag2': log_stock.shift(2),\n",
    "        'lag3': log_stock.shift(3),\n",
    "        'moving_avg_3': log_stock.rolling(window=3).mean().shift(1),\n",
    "        'moving_avg_5': log_stock.rolling(window=5).mean().shift(1),\n",
    "        'moving_avg_10': log_stock.rolling(window=10).mean().shift(1),\n",
    "        'std_5': log_stock.rolling(window=5).std().shift(1),\n",
    "        'std_10': log_stock.rolling(window=10).std().shift(1),\n",
    "        'max_5': log_stock.rolling(window=5).max().shift(1),\n",
    "        'min_5': log_stock.rolling(window=5).min().shift(1)})\n",
    "\n",
    "    gb_data = gb_data.dropna()\n",
    "\n",
    "    number = len(gb_data)\n",
    "    train_size = int(number * 0.8)\n",
    "    validation_size = int(number * 0.1)\n",
    "\n",
    "    gb_train = gb_data.iloc[:train_size]\n",
    "    gb_validation = gb_data.iloc[train_size:train_size + validation_size]\n",
    "    gb_test = gb_data.iloc[train_size + validation_size:]\n",
    "\n",
    "    feature_columns = ['trend', 'lag1', 'lag2', 'lag3', 'moving_avg_3', 'moving_avg_5', 'moving_avg_10', 'std_5', 'std_10', 'max_5', 'min_5']\n",
    "    \n",
    "    train_X = gb_train[feature_columns]\n",
    "    train_y = gb_train['volatility']\n",
    "    validation_X = gb_validation[feature_columns]\n",
    "    validation_y = gb_validation['volatility']\n",
    "    test_X = gb_test[feature_columns]\n",
    "    test_y = gb_test['volatility']\n",
    "\n",
    "    final_MAPE = float('inf')\n",
    "    final_Prediction = None\n",
    "    final_Actual = None\n",
    "    best_lr = None  # Track best learning rate\n",
    "\n",
    "    for learning in learning_rate:\n",
    "        gb_model = GradientBoostingRegressor(n_estimators=500, learning_rate=learning, max_depth=6, random_state=42)\n",
    "        gb_model.fit(train_X, train_y)\n",
    "        validation_Prediction = np.exp(gb_model.predict(validation_X))\n",
    "        validation_Actual = np.exp(validation_y)\n",
    "        validation_RMSE, validation_QLIKE, validation_RMPSE, validation_MAPE = metric(validation_Actual, validation_Prediction)\n",
    "\n",
    "        if validation_MAPE < final_MAPE:\n",
    "            final_MAPE = validation_MAPE\n",
    "            final_Prediction = np.exp(gb_model.predict(test_X))\n",
    "            final_Actual = np.exp(test_y)\n",
    "            best_lr = learning  # Store best learning rate\n",
    "\n",
    "    gb_panel[stock] = pd.Series(final_Prediction, index=gb_test.index)\n",
    "\n",
    "    RMSE, QLIKE, RMPSE, MAPE = metric(final_Prediction, final_Actual)\n",
    "    MAPE_list.append(MAPE)\n",
    "    RMSE_list.append(RMSE)\n",
    "    QLIKE_list.append(QLIKE)\n",
    "    RMPSE_list.append(RMPSE)\n",
    "    \n",
    "    # Store best learning rate in the dictionary\n",
    "    best_lr_dict[stock] = best_lr\n",
    "\n",
    "print_summary(RMSE_list, QLIKE_list, RMPSE_list, MAPE_list, 'Gradient Boosting')\n",
    "\n",
    "# Print best learning rates for each stock\n",
    "print(\"\\nBest Learning Rates for Gradient Boosting Model:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Stock':<15} {'Learning Rate':<15}\")\n",
    "print(\"-\" * 40)\n",
    "for stock, lr in best_lr_dict.items():\n",
    "    print(f\"{stock:<15} {lr:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54235fbc",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe65e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store value\n",
    "linear_panel = pd.DataFrame()\n",
    "QLIKE_list = []\n",
    "RMSE_list = []\n",
    "RMPSE_list = []\n",
    "MAPE_list = []\n",
    "# Dictionary to store best moving average for each stock\n",
    "best_ma_dict = {}\n",
    "\n",
    "# Hyperparameters tuning\n",
    "moving_average = [3, 5, 7, 10]\n",
    "\n",
    "# go through each stock\n",
    "for stock in real_volatility.columns:  \n",
    "    stock_series = real_volatility[stock]\n",
    "    log_stock = np.log(stock_series)\n",
    "\n",
    "    # initialise for the best performance tracker\n",
    "    final_MAPE = float('inf')\n",
    "    final_Prediction = None\n",
    "    final_Actual = None\n",
    "    best_ma = None  # Track best moving average window\n",
    "\n",
    "    # go through each moving average threshold\n",
    "    for moving in moving_average: \n",
    "\n",
    "        # feature sets\n",
    "        linear_data = pd.DataFrame({\n",
    "            'volatility': log_stock,\n",
    "            'trend': np.arange(len(log_stock)),\n",
    "            'lag1': log_stock.shift(1),\n",
    "            'lag2': log_stock.shift(2),\n",
    "            'moving_avg': log_stock.rolling(window=moving).mean().shift(1)})\n",
    "        linear_data = linear_data.dropna()\n",
    "\n",
    "        # Train-Validation-Test split\n",
    "        number = len(linear_data)\n",
    "        train_size = int(number * 0.8)\n",
    "        validation_size = int(number * 0.1)\n",
    "\n",
    "        linear_train = linear_data.iloc[:train_size]\n",
    "        linear_validation = linear_data.iloc[train_size:train_size + validation_size]\n",
    "        linear_test = linear_data.iloc[train_size + validation_size:]\n",
    "\n",
    "        # Prepare the dataset \n",
    "        feature_cols = ['trend', 'lag1', 'lag2', 'moving_avg']\n",
    "        train_X = linear_train[feature_cols]\n",
    "        train_y = linear_train['volatility']\n",
    "        validation_X = linear_validation[feature_cols]\n",
    "        validation_y = linear_validation['volatility']\n",
    "        test_X = linear_test[feature_cols]\n",
    "        test_y = linear_test['volatility'] \n",
    "\n",
    "        # fit on train set\n",
    "        linear_model = LinearRegression()\n",
    "        linear_model.fit(train_X, train_y)\n",
    "\n",
    "        # evaluate on the validation set\n",
    "        validation_Prediction = np.exp(linear_model.predict(validation_X))\n",
    "        validation_Actual = np.exp(validation_y)\n",
    "        validation_RMSE, validation_QLIKE, validation_RMPSE, validation_MAPE = metric(validation_Actual, validation_Prediction)\n",
    "\n",
    "        # check is this the best\n",
    "        if validation_MAPE < final_MAPE:\n",
    "            final_MAPE = validation_MAPE\n",
    "            final_Prediction = np.exp(linear_model.predict(test_X))\n",
    "            final_Actual = np.exp(test_y)\n",
    "            best_ma = moving  # Store the best moving average\n",
    "\n",
    "    # store the value for each stock\n",
    "    if final_Prediction is not None:\n",
    "        linear_panel[stock] = pd.Series(final_Prediction, index=linear_test.index)\n",
    "\n",
    "        RMSE, QLIKE, RMPSE, MAPE = metric(final_Prediction, final_Actual)\n",
    "        MAPE_list.append(MAPE)\n",
    "        RMSE_list.append(RMSE)\n",
    "        QLIKE_list.append(QLIKE)\n",
    "        RMPSE_list.append(RMPSE)\n",
    "        \n",
    "        # Store best moving average window in the dictionary\n",
    "        best_ma_dict[stock] = best_ma\n",
    "\n",
    "# Print summary\n",
    "print_summary(RMSE_list, QLIKE_list, RMPSE_list, MAPE_list, 'Linear Regression')\n",
    "\n",
    "# Print best moving average windows for each stock\n",
    "print(\"\\nBest Moving Average Windows for Linear Regression Model:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Stock':<15} {'Moving Average':<15}\")\n",
    "print(\"-\" * 40)\n",
    "for stock, ma in best_ma_dict.items():\n",
    "    print(f\"{stock:<15} {ma:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509335ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use current directory as save path\n",
    "save_path = Path('.')\n",
    "\n",
    "# All model panels\n",
    "models_panels = {\n",
    "    'LAG': lag_panel,\n",
    "    'HAR_RV': har_panel, \n",
    "    'Linear': linear_panel,\n",
    "    'PCA_Linear': pca_linear_panel,\n",
    "    'Random_Forest': rf_panel,\n",
    "    'Gradient_Boosting': gb_panel\n",
    "}\n",
    "\n",
    "# 1. Save all model panels to CSV\n",
    "print(\"Saving model panels to current directory...\")\n",
    "for model_name, panel in models_panels.items():\n",
    "    filename = f\"{model_name}_predictions.csv\"\n",
    "    filepath = save_path / filename\n",
    "    panel.to_csv(filepath)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "# 2. Calculate metrics for all models and generate table\n",
    "def calculate_metrics(predictions, actuals):\n",
    "    # Use the metric function defined at the top of the file\n",
    "    # which already returns RMSE, QLIKE, RMPSE, MAPE\n",
    "    return metric(predictions, actuals)\n",
    "\n",
    "# Get common index and columns\n",
    "common_index = None\n",
    "common_stocks = None\n",
    "for model_name, panel in models_panels.items():\n",
    "    if common_index is None:\n",
    "        common_index = panel.index\n",
    "        common_stocks = panel.columns\n",
    "    else:\n",
    "        common_index = common_index.intersection(panel.index)\n",
    "        common_stocks = common_stocks.intersection(panel.columns)\n",
    "\n",
    "# Align actual values\n",
    "aligned_real = real_volatility.loc[common_index, common_stocks]\n",
    "\n",
    "# Calculate metrics for each model\n",
    "metrics_data = []\n",
    "for model_name, panel in models_panels.items():\n",
    "    aligned_pred = panel.loc[common_index, common_stocks]\n",
    "    \n",
    "    pred_values = aligned_pred.values.flatten()\n",
    "    real_values = aligned_real.values.flatten()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    mask = ~(np.isnan(pred_values) | np.isnan(real_values))\n",
    "    pred_clean = pred_values[mask]\n",
    "    real_clean = real_values[mask]\n",
    "    \n",
    "    rmse, qlike, rmpse_val, mape_val = calculate_metrics(pred_clean, real_clean)\n",
    "    metrics_data.append({\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'QLIKE': qlike,\n",
    "        'RMPSE': rmpse_val,\n",
    "        'MAPE': mape_val\n",
    "    })\n",
    "\n",
    "# Create metrics table\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df = metrics_df.round(6)\n",
    "\n",
    "# Save metrics table\n",
    "metrics_df.to_csv(save_path / \"model_metrics_summary.csv\", index=False)\n",
    "\n",
    "# Display table\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(\"=\"*60)\n",
    "print(metrics_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
