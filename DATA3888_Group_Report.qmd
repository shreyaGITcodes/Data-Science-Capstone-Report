---
title: "DATA3888 Group Report"
author: 
  - name: "Shreya Prakash (520496062)"
  - name: "Chenuka (530080640)"
  - name: "Binh Minh Tran (530414672)"
  - name: "Enoch Wong (530531430)"
  - name: "Ruohai (540222281)"
  - name: "Zoha (530526838)"
format:
  html:
    toc: true
    toc_float: true
    toc-depth: 1
    code-fold: true
    code-default: false
    code-summary: "Show Code"
    embed-resources: true
editor: visual
bibliography: references.bib
---

## 1. Executive Summary

-   Short description of the problem.

-   The main findings.

-   Key figure if appropriate.

-   The practical relevance of the analysis.

## 2. Introduction

2.1 Market importance of RV

2.2 Limitations of traditional models 

2.3 Aim & contributions

## 3. Data

3.1 Optiver LOB dataset \>\>\>\>\>

```{python}
# list all book files and the target table
from glob import glob
import pandas as pd, os

book_paths = sorted(glob("individual_book_train/stock_*.csv"))

df_files = (pd.DataFrame({"path": book_paths})
              .assign(stock_id=lambda d: d["path"]
                      .str.extract(r'(\d+)').astype(int)))
targets = pd.read_csv("Optiver_additional data/train.csv")
print(f"{len(df_files)} stocks | {targets.time_id.nunique()} time buckets")
```

3.2 De-normalization & cleaning

From our literature review we found that the kaggle discussion threads revealed that the prices were scaled by an unknown divisor D and then rounded to the nearest real market tick size (\~ \$0.01). For every `(stock_id, time_id)` we, forward fill the `600` snapshots so that every second has a quote. Compute first the differences in the price $\delta P = price_t - price_{t-1}$ and find the smallest non zero absolute jump; that equals $\frac{1 \text{tick}}{D}$ then multiply the whole bucket by $\frac{0.01}{\text{min}(|\delta P_{norm}|)}$. We get the real prices by doing $P^{\text{real}}_t = D \times P^{\text{norm}}_t$

```{python}
def denorm_scale(grp, col="ask_price1"):
    grp = (grp.set_index("seconds_in_bucket")
              .reindex(range(600), method="ffill")
              .reset_index())
    tick = grp[col].diff().abs().loc[lambda x: x>0].min()
    return (grp[col] * (0.01 / tick)).mean()       # bucket opening price

prices = []
for _, row in df_files.iterrows():
    df = pd.read_csv(row.path, usecols=["time_id", "seconds_in_bucket", "ask_price1"])
    s  = df.groupby("time_id").apply(denorm_scale, include_groups=False).rename(row.stock_id)
    prices.append(s)

df_prices = pd.concat(prices, axis=1)   # rows: time_id, cols: stock_id
```

The resulting `3830 x 120` matrix is our master price panel. A quick histogram of $\delta P$ by `tick` show exactly integers only which confirms to us the re-scaling recovered genuine tick units.

### 3.2.2 Handling the gaps and extreme quotes

Similar to earlier we used forward / backward to impute the remaining holes with the last known quotes; this preserves the micros structure dynamics without fabricating new trends and loosing generality of our method. We exclude a stock if more than 0.05 % of its 1-second snapshots are missing on any trading day (≈ 44 of 88 200). This ceiling keeps the expected gap below 1 s in a 10-minute bucket, ensuring forward-fill imputation cannot materially flatten high-frequency dynamics. To prevent single tick glitches we from exploding volatility estimates we winsorise each stocks price at the 0.1 % and 99.9% of the quantiles.

```{python}
# remove excessively sparse columns
from sklearn.manifold import SpectralEmbedding
from sklearn.preprocessing import minmax_scale
import yfinance as yf

def spectral_order(df, k=30, seed=42):
    """
    Return index sorted by the leading spectral coordinate.
    df : (n_buckets × n_stocks) price matrix with *no* NaNs.
    """
    X = minmax_scale(df.values)                                         # 1 normalise
    coord = (SpectralEmbedding(n_components=1,
                               n_neighbors=min(k, len(df)-1),
                               random_state=seed)
             .fit_transform(X).ravel())                                 # 2 embed
    return df.index[coord.argsort()]                                    # 3 sort

time_id_ordered = spectral_order(df_prices_denorm_clean)

THRESHOLD = 0.0005
keep = df_prices.isna().mean().le(0.0005)
df_prices = df_prices.loc[:, keep]

# winsorise
q_lo, q_hi = df_prices.quantile(0.001), df_prices.quantile(0.999)
df_prices_denorm_clean = df_prices.clip(lower=q_lo, upper=q_hi, axis=1).ffill().bfill()
df_prices_ordered = df_prices_denorm_clean.reindex(time_id_ordered)
```

This now underpins all subsequent features. To finally recover the chronological order of the `time_ids` to improve the per bucket RV prediction we embedded each bucket in a 1-D spectral manifold and sort by the leading eigen-coordinate. Because prices evolve almost monotonically intra-day, the leading spectral component monotonises the shuffled ids, effectively restoring the hidden chronology. We validate the approach by applying the same embedding to daily closing prices of the S&P-100 (right panel in Figure 1); the recovered order aligns perfectly with calendar dates, confirming the method’s fidelity.

```{python}
from sklearn.manifold import SpectralEmbedding
from sklearn.preprocessing import minmax_scale
import yfinance as yf

import yfinance as yf, pandas as pd, numpy as np, matplotlib.pyplot as plt
import matplotlib.dates as mdates

# Download daily S&P-100 closes for a visual benchmark
sp100 = pd.read_html("https://en.wikipedia.org/wiki/S%26P_100")[2].Symbol
df_real = (yf.download(sp100.to_list(), start="2020-01-01", end="2021-06-01",
                       interval="1d")['Close']
             .dropna(axis=1, thresh=0.5*len(sp100))
             .dropna())                              

# embed both matrices in 2-D for eyeballing
embed = SpectralEmbedding(n_components=2, random_state=42)
Z_denorm = embed.fit_transform(minmax_scale(df_prices_ordered.values))
Z_real   = embed.fit_transform(minmax_scale(df_real.values))

# Plot: colour = recovered order (Optiver)  vs calendar date (S&P)
fig, ax = plt.subplots(1, 2, figsize=(14, 6))
sc0 = ax[0].scatter(Z_denorm[:, 0], Z_denorm[:, 1],
                    c=np.arange(len(Z_denorm)), cmap='viridis', s=8)
ax[0].set_title("Optiver buckets – colour = spectral order")
fig.colorbar(sc0, ax=ax[0], shrink=0.7)
sc1 = ax[1].scatter(Z_real[:, 0], Z_real[:, 1],
                    c=mdates.date2num(df_real.index), cmap='viridis', s=8)
ax[1].set_title("S&P-100 daily – colour = calendar date")
fig.colorbar(sc1, ax=ax[1], shrink=0.7)
fig.tight_layout()
plt.show()
```

#### 3.2.3 Assesing Characteristics and trends

After recovering the chornology of the time_ids, for each stock we calcualted their RV across this time_id trend and then calculated the average RV per time_id from all the stocks, plotting a Averaged RV against time_id. Below is the trend of the data we observed.

```{python}
def compute_rv(grp):
    """Compute realized volatility from intraday price data"""
    # Calculate WAP (Weighted Average Price) if bid/ask data available
    if all(col in grp.columns for col in ['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1']):
        wap = (grp['bid_price1'] * grp['ask_size1'] + 
               grp['ask_price1'] * grp['bid_size1']) / \
              (grp['bid_size1'] + grp['ask_size1'])
    else:
        # Fallback to ask_price1 if WAP components not available
        wap = grp['ask_price1']
    
    # Calculate log returns and realized volatility
    log_returns = np.log(wap).diff().dropna()
    rv = np.sqrt((log_returns ** 2).sum())
    return rv

# Calculate RV for each stock and time_id combination
rv_records = []
for _, row in df_files.iterrows():
    try:
        # Load the individual book file
        dfb = pd.read_csv(row.path)
        
        # Calculate RV for each time_id in this stock
        rv_series = (dfb.groupby('time_id')
                       .apply(compute_rv, include_groups=False)
                       .rename('rv')
                       .reset_index())
        rv_series['stock_id'] = row.stock_id
        rv_records.append(rv_series)
    except Exception as e:
        print(f"Error processing {row.path}: {e}")
        continue

# Combine all RV calculations
rv_df = pd.concat(rv_records, ignore_index=True)

# Map to bucket_idx using time ordering
time_map = pd.DataFrame({'time_id': time_id_ordered})
time_map['bucket_idx'] = range(len(time_map))
rv_df = rv_df.merge(time_map, on='time_id')

# rv_df = pd.DataFrame(rv_data)

# Create pivot table for analysis
rv_pivot = rv_df.pivot(index='bucket_idx', columns='stock_id', values='rv')

# Calculate average RV across all stocks for each time bucket
avg_rv = rv_pivot.mean(axis=1)

# Plot average RV trend
plt.figure(figsize=(12, 6))
avg_rv.plot(title="Average Realised Volatility Over Time", 
           xlabel="Bucket Index", ylabel="Average RV")
plt.tight_layout()
plt.show()
```

```{python}
rv_pivot
```

$$
     \text{avg\_RV}_t \;=\;\frac1N\sum_{i=1}^N \mathrm{RV}_{t,i}.
   $$

One of the main concern about time series data is on non-stationarity. While test like Augmented Dickey-Fuller exists to test the statistical significance of this it fails when there are breaks. So we decided to use Zivot–Andrews Statistic for this reason to test for non-stationarity. Below is the code for this.

```{python}
# Stationarity tests
from statsmodels.tsa.stattools import adfuller, zivot_andrews

# ADF test as baseline
adf_stat, adf_p, *_ = adfuller(avg_rv.dropna())
print(f"ADF test → statistic: {adf_stat:.3f}, p-value: {adf_p:.3f}")

# Zivot–Andrews test for structural breaks
za_stat, za_pval, za_cv, za_lag, za_bpidx = zivot_andrews(
    avg_rv.dropna().values,
    regression='ct',  # break in both intercept and trend
    trim=0.15,
    maxlag=12,
    autolag='AIC'
)

print(f"Zivot–Andrews Statistic: {za_stat:.4f}")
print(f"p-value: {za_pval:.4f}")
print("Critical values:")
for sig, cv in za_cv.items():
    print(f"  {sig} : {cv:.4f}")
print(f"Lags used in regression: {za_lag}")
print(f"Estimated break at index: {za_bpidx}")
```

------- transform data

```{python}
# Add this after your existing avg_rv calculation
from statsmodels.tsa.stattools import zivot_andrews

# Test different stationarity transformations on YOUR actual data using Zivot-Andrews
def test_stationarity(ts, name="Series"):
    """Test stationarity using Zivot-Andrews test (better for structural breaks)"""
    ts_clean = ts.dropna()
    
    try:
        za_stat, za_pval, za_cv, za_lag, za_bpidx = zivot_andrews(
            ts_clean.values,
            regression='ct',  # break in both intercept and trend
            trim=0.15,
            maxlag=12,
            autolag='AIC'
        )
        
        # Check against 5% critical value
        is_stationary = za_stat < za_cv['5%']
        status = '✓ Stationary' if is_stationary else '✗ Non-stationary'
        
        print(f"{name:15} | ZA: {za_stat:6.3f} (p={za_pval:.3f}) | {status}")
        print(f"{'':15} | Break at index: {za_bpidx} | 5% crit: {za_cv['5%']:6.3f}")
        
        return is_stationary
        
    except Exception as e:
        print(f"{name:15} | ZA test failed: {str(e)}")
        return False

print("Testing stationarity transformations on your Optiver data (Zivot-Andrews):")
print("=" * 75)

# Test original
test_stationarity(avg_rv, "Original")

# 1. Log + First Difference (recommended for volatility)
log_rv = np.log(avg_rv + 1e-8)
stationary_rv = log_rv.diff().dropna()
is_stationary = test_stationarity(stationary_rv, "Log + Diff")

# 2. Alternative: Log + Rolling Z-score  
window = 100
log_rolling_mean = log_rv.rolling(window=window, center=True).mean()
log_rolling_std = log_rv.rolling(window=window, center=True).std()
stationary_rv_alt = ((log_rv - log_rolling_mean) / log_rolling_std).dropna()
test_stationarity(stationary_rv_alt, "Log + Z-score")

# 3. Simple first difference
simple_diff = avg_rv.diff().dropna()
test_stationarity(simple_diff, "Simple Diff")

# 4. Percentage change
pct_change = avg_rv.pct_change().dropna()
test_stationarity(pct_change, "Pct Change")

# Use the best transformation for your GAT model
if is_stationary:
    final_stationary = stationary_rv
    transform_method = "log_diff"
    print(f"\n✅ Using log+diff transformation: {len(stationary_rv)} time points")
else:
    print(f"\n⚠️  Testing other transformations...")
    # Test which one works best
```

----- plots for transformed data

```{python}
import matplotlib.pyplot as plt
import numpy as np

# After running your transformation code, plot the results
def plot_volatility_transformation(avg_rv, transformed_data, transform_name="Log + Diff"):
    """
    Plot original vs transformed volatility data
    
    Parameters:
    avg_rv: original average RV series
    transformed_data: stationary transformed data
    transform_name: name of transformation for title
    """
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    
    # Plot 1: Original data
    axes[0].plot(avg_rv.index, avg_rv.values, color='blue', linewidth=0.8)
    axes[0].set_title('Original Average Realised Volatility Over Time', fontsize=14)
    axes[0].set_ylabel('Average RV', fontsize=12)
    axes[0].grid(True, alpha=0.3)
    
    # Add annotation for the volatility spike
    spike_idx = avg_rv.idxmax()
    spike_val = avg_rv.max()
    axes[0].annotate(f'Max volatility spike\n(Bucket {spike_idx})', 
                    xy=(spike_idx, spike_val), 
                    xytext=(spike_idx + 300, spike_val - 0.3),
                    arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),
                    fontsize=10, color='red')
    
    # Plot 2: Transformed data
    # Handle index adjustment for transformations that reduce length
    if len(transformed_data) == len(avg_rv):
        transform_index = avg_rv.index
    else:
        # For first difference, start from index 1
        transform_index = avg_rv.index[1:len(transformed_data)+1]
    
    axes[1].plot(transform_index, transformed_data.values, color='green', linewidth=0.8)
    axes[1].set_title(f'Transformed Volatility ({transform_name}) - Stationary', fontsize=14)
    axes[1].set_ylabel(f'Transformed RV ({transform_name})', fontsize=12)
    axes[1].set_xlabel('Bucket Index', fontsize=12)
    axes[1].grid(True, alpha=0.3)
    
    # Add horizontal line at zero for transformed data
    axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.show()
    
    # Print some summary statistics
    print(f"Original data statistics:")
    print(f"  Mean: {avg_rv.mean():.4f}, Std: {avg_rv.std():.4f}")
    print(f"  Min: {avg_rv.min():.4f}, Max: {avg_rv.max():.4f}")
    print(f"  Length: {len(avg_rv)}")
    
    print(f"\nTransformed data statistics ({transform_name}):")
    print(f"  Mean: {transformed_data.mean():.4f}, Std: {transformed_data.std():.4f}")
    print(f"  Min: {transformed_data.min():.4f}, Max: {transformed_data.max():.4f}")
    print(f"  Length: {len(transformed_data)}")

# Apply this after your transformation code:

# For log + first difference
log_rv = np.log(avg_rv + 1e-8)
stationary_rv = log_rv.diff().dropna()
plot_volatility_transformation(avg_rv, stationary_rv, "Log + First Diff")

# For log + rolling z-score (if you want to compare)
window = 100
log_rolling_mean = log_rv.rolling(window=window, center=True).mean()
log_rolling_std = log_rv.rolling(window=window, center=True).std()
stationary_rv_zscore = ((log_rv - log_rolling_mean) / log_rolling_std).dropna()
plot_volatility_transformation(avg_rv, stationary_rv_zscore, "Log + Rolling Z-score")

# Alternative: Side-by-side comparison
def plot_side_by_side_comparison(avg_rv, transformations_dict):
    """
    Plot multiple transformations side by side for comparison
    
    Parameters:
    avg_rv: original data
    transformations_dict: dict of {"name": transformed_series}
    """
    
    n_transforms = len(transformations_dict)
    fig, axes = plt.subplots(1, n_transforms + 1, figsize=(5 * (n_transforms + 1), 6))
    
    # Plot original
    axes[0].plot(avg_rv.index, avg_rv.values, color='blue', linewidth=0.8)
    axes[0].set_title('Original\nAverage RV', fontsize=12)
    axes[0].set_ylabel('Average RV')
    axes[0].grid(True, alpha=0.3)
    
    # Plot each transformation
    colors = ['green', 'red', 'orange', 'purple', 'brown']
    for i, (name, data) in enumerate(transformations_dict.items()):
        # Handle index adjustment
        if len(data) == len(avg_rv):
            plot_index = avg_rv.index
        else:
            plot_index = avg_rv.index[1:len(data)+1]
            
        axes[i+1].plot(plot_index, data.values, color=colors[i % len(colors)], linewidth=0.8)
        axes[i+1].set_title(f'{name}\n(Stationary)', fontsize=12)
        axes[i+1].set_ylabel('Transformed RV')
        axes[i+1].grid(True, alpha=0.3)
        axes[i+1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
    
    # Set common x-label
    for ax in axes:
        ax.set_xlabel('Bucket Index')
    
    plt.tight_layout()
    plt.show()

# Example usage for multiple transformations:
transformations = {
    "Log + Diff": stationary_rv,
    "Log + Z-score": stationary_rv_zscore,
    "Simple Diff": avg_rv.diff().dropna(),
    "Pct Change": avg_rv.pct_change().dropna()
}

plot_side_by_side_comparison(avg_rv, transformations)
```

```{python}
stationary_rv.head()
```

----- making the new RV panel with transform
```{python}
import numpy as np
import pandas as pd

def create_stationary_features_fixed(rv_pivot, window=100):
    """
    Create stationary features WITHOUT future leakage
    """
    print("🔧 Creating stationary features without data leakage...")
    
    if isinstance(rv_pivot, pd.DataFrame):
        rv_matrix = rv_pivot.values
    else:
        rv_matrix = rv_pivot
    
    T, N = rv_matrix.shape
    print(f"Input matrix: {T} time points × {N} stocks")
    
    # Method 1: Simple log + first difference (RECOMMENDED)
    log_rv = np.log(rv_matrix + 1e-8)
    stationary_rv = np.diff(log_rv, axis=0)  # Shape: (T-1, N)
    
    # Method 2: Alternative - Rolling z-score WITHOUT center=True
    # rolling_mean = pd.DataFrame(log_rv).rolling(window=window, min_periods=1).mean().values
    # rolling_std = pd.DataFrame(log_rv).rolling(window=window, min_periods=1).std().values
    # stationary_rv_alt = (log_rv - rolling_mean) / (rolling_std + 1e-8)
    
    # Clean up any remaining NaNs/infs
    stationary_rv = np.nan_to_num(stationary_rv, nan=0.0, posinf=0.0, neginf=0.0)
    
    print(f"Stationary matrix: {stationary_rv.shape}")
    print(f"Stats - Mean: {np.mean(stationary_rv):.4f}, Std: {np.std(stationary_rv):.4f}")
    print(f"Range: [{np.min(stationary_rv):.4f}, {np.max(stationary_rv):.4f}]")
    
    return stationary_rv

transformed_rv_pivot = create_stationary_features_fixed(rv_pivot, window=100)

# Verify the transformation worked
print("\n" + "="*60)
print("TRANSFORMATION VERIFICATION")
print("="*60)

# Check statistics before and after
print(f"Original rv_pivot:")
if isinstance(rv_pivot, pd.DataFrame):
    orig_values = rv_pivot.values
else:
    orig_values = rv_pivot
    
print(f"  Shape: {orig_values.shape}")
print(f"  Mean: {np.nanmean(orig_values):.4f}, Std: {np.nanstd(orig_values):.4f}")
print(f"  Min: {np.nanmin(orig_values):.4f}, Max: {np.nanmax(orig_values):.4f}")
print(f"  NaN count: {np.isnan(orig_values).sum()}")

print(f"\nTransformed rv_pivot:")
print(f"  Shape: {transformed_rv_pivot.shape}")
print(f"  Mean: {np.nanmean(transformed_rv_pivot):.4f}, Std: {np.nanstd(transformed_rv_pivot):.4f}")
print(f"  Min: {np.nanmin(transformed_rv_pivot):.4f}, Max: {np.nanmax(transformed_rv_pivot):.4f}")
print(f"  NaN count: {np.isnan(transformed_rv_pivot).sum()}")

# Test stationarity on a few random stocks
from statsmodels.tsa.stattools import zivot_andrews

def test_stock_stationarity(rv_matrix, stock_indices=None, max_stocks=5):
    """Test stationarity for individual stocks"""
    if stock_indices is None:
        stock_indices = np.random.choice(rv_matrix.shape[1], min(max_stocks, rv_matrix.shape[1]), replace=False)
    
    print(f"\nTesting stationarity for {len(stock_indices)} random stocks:")
    print("-" * 50)
    
    stationary_count = 0
    for i, stock_idx in enumerate(stock_indices):
        stock_data = rv_matrix[:, stock_idx]
        stock_data_clean = stock_data[~np.isnan(stock_data)]
        
        if len(stock_data_clean) > 50:  # minimum data points for ZA test
            try:
                za_stat, za_pval, za_cv, _, _ = zivot_andrews(
                    stock_data_clean, regression='ct', trim=0.15, maxlag=12, autolag='AIC'
                )
                is_stationary = za_stat < za_cv['5%']
                status = '✓ Stationary' if is_stationary else '✗ Non-stationary'
                print(f"Stock {stock_idx:3d}: ZA={za_stat:6.3f} (p={za_pval:.3f}) {status}")
                if is_stationary:
                    stationary_count += 1
            except:
                print(f"Stock {stock_idx:3d}: ZA test failed")
        else:
            print(f"Stock {stock_idx:3d}: Insufficient data")
    
    print(f"\nStationarity rate: {stationary_count}/{len(stock_indices)} stocks")
    return stationary_count / len(stock_indices)

# Test original vs transformed
print("\nOriginal rv_pivot stationarity:")
orig_stat_rate = test_stock_stationarity(orig_values)

print("\nTransformed rv_pivot stationarity:")
trans_stat_rate = test_stock_stationarity(transformed_rv_pivot)

print(f"\n📊 IMPROVEMENT: {trans_stat_rate:.1%} stationary (vs {orig_stat_rate:.1%} original)")

# Update your existing pipeline to use transformed data
print("\n" + "="*60)
print("UPDATING YOUR GAT PIPELINE")
print("="*60)
```


## 4. Methodology

4.1 Feature engineering

### 4.2 Graph construction & GAT architecture

In this section we describe how we turned the above denormaised price matrix into a static stock stock-neighbour graph, assemble node-level features and the apply a two layer Graph Attention Network for bucket‐by‐bucket volatility forecasting.

#### 4.2.1 Building the neighbour graph and GAT

After the pruning and aligning of the stocks with the threshold dropp rate of 0.01% on missing buckets, for each stock we processed to analyse the neighbours by a T length price trajectory. Each stock is a point in $\mathbb{R}^T$. We Min-Max scale and query a KD-tree.

```{python}
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KDTree
import torch
import numpy as np

K = 3  # number of neighbours per node

# Convert rv_pivot to numpy array if needed
if isinstance(rv_pivot, pd.DataFrame):
    rv_matrix = rv_pivot.values
else:
    rv_matrix = rv_pivot

# Transpose to (N stocks × T time_points) for KDTree
X_volatility = rv_matrix.T  
print(f"Building volatility-based graph: {X_volatility.shape} (N × T)")

# Remove stocks with missing data
valid_stocks = ~np.isnan(X_volatility).any(axis=1)
X_clean = X_volatility[valid_stocks]
valid_indices = np.where(valid_stocks)[0]
print(f"Valid stocks: {len(valid_indices)} / {len(valid_stocks)}")

# Min-Max scale and build KDTree
X_scaled = MinMaxScaler().fit_transform(X_clean)
tree = KDTree(X_scaled, metric='euclidean')
dist, nbr_raw = tree.query(X_scaled, k=K+1)  # includes self at index 0

# Map back to original stock indices
nbr = valid_indices[nbr_raw]

# Build edge index & weights (exclude self-loops)
src = np.repeat(valid_indices, K)
dst = nbr[:, 1:].ravel()
edge_index = torch.tensor([src, dst], dtype=torch.long)
edge_weight = torch.exp(-torch.tensor(dist[:, 1:].ravel(), dtype=torch.float))

print(f"Graph: {len(valid_indices)} nodes, {len(edge_index[0])} edges")

```


----- updating the features ! include 5 LOB features insetad of volatility momentum
For each bucket $t$ and stock $i$ we then build node level features which include Own-RV lags (lags 1, 2, 3), Mean neighbour RV and other order‐book micro-features.

```{python}

# Now update your feature engineering to use the transformed rv_pivot
def build_features_fixed(stationary_rv, neighbor_indices):
    """
    Build features properly with correct dimensions
    """
    T, N = stationary_rv.shape
    
    # 1. Own-RV lags (3 features)
    lags = [1, 2, 3]
    own_feats = np.zeros((T, N, len(lags)))
    
    for i, lag in enumerate(lags):
        if lag < T:
            own_feats[lag:, :, i] = stationary_rv[:-lag, :]
    
    # 2. Neighbor mean RV (1 feature)
    nei_mean = np.zeros((T, N, 1))
    for i in range(N):
        if i < len(neighbor_indices) and len(neighbor_indices[i]) > 1:
            nei_idx = neighbor_indices[i, 1:]  # exclude self
            nei_idx = nei_idx[nei_idx < N]  # ensure valid indices
            if len(nei_idx) > 0:
                nei_mean[:, i, 0] = stationary_rv[:, nei_idx].mean(axis=1)
    
    # 3. Volatility momentum (1 feature)
    vol_momentum = np.zeros((T, N, 1))
    for t in range(6, T):
        recent = np.mean(stationary_rv[t-3:t], axis=0)
        past = np.mean(stationary_rv[t-6:t-3], axis=0)
        vol_momentum[t, :, 0] = recent - past
    
    # Concatenate features
    X = np.concatenate([own_feats, nei_mean, vol_momentum], axis=2)  # Shape: (T, N, 5)
    
    # Clean up
    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
    
    print(f"✅ Features created: {X.shape} (T × N × {X.shape[2]} features)")
    
    return X, stationary_rv

# Create features using transformed data
X_transformed, y_transformed = build_features_fixed(transformed_rv_pivot, nbr)

print(f"✅ Features created from transformed data:")
print(f"   X shape: {X_transformed.shape} (T × N × Features)")
print(f"   y shape: {y_transformed.shape} (T × N)")
print(f"   Features per node: {X_transformed.shape[2]}")

# Convert to tensors for GAT
import torch
X_tensor_transformed = torch.tensor(X_transformed, dtype=torch.float)
y_tensor_transformed = torch.tensor(y_transformed, dtype=torch.float)

print(f"\n🎯 Ready for GAT training with transformed stationary data!")
print(f"   Input tensor: {X_tensor_transformed.shape}")
print(f"   Target tensor: {y_tensor_transformed.shape}")

# Save transformation parameters for later use
print(f"\n💾 Transformation parameters saved for inverse transformation during evaluation")
```

Then to learn the dynamic weightninigs of the neighbiurse we deine a simple 2-layer GAT that lets each stock fuse its own history with weighted neighbour signals.

```{python}
import torch
import torch.nn.functional as F
from torch_geometric.nn import GATConv

class ImprovedVolatilityGAT(torch.nn.Module):
    def __init__(self, in_feats, hidden=64, heads=4, dropout=0.3):
        super().__init__()
        self.dropout = dropout
        
        # Smaller, more stable architecture
        self.conv1 = GATConv(in_feats, hidden, heads=heads, dropout=dropout, concat=True)
        self.conv2 = GATConv(hidden * heads, hidden // 2, heads=2, dropout=dropout, concat=True)
        self.conv3 = GATConv(hidden, 1, heads=1, concat=False, dropout=dropout)
        
        # Batch normalization for stability
        self.bn1 = torch.nn.BatchNorm1d(hidden * heads)
        self.bn2 = torch.nn.BatchNorm1d(hidden)
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """Better weight initialization"""
        for m in self.modules():
            if isinstance(m, torch.nn.Linear):
                torch.nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    torch.nn.init.constant_(m.bias, 0)
    
    def forward(self, x, edge_index):
        # Layer 1
        h = self.conv1(x, edge_index)
        h = self.bn1(h) if h.size(0) > 1 else h  # Skip BN for single samples
        h = F.elu(h)
        h = F.dropout(h, p=self.dropout, training=self.training)
        
        # Layer 2
        h = self.conv2(h, edge_index)
        h = self.bn2(h) if h.size(0) > 1 else h
        h = F.elu(h)
        h = F.dropout(h, p=self.dropout, training=self.training)
        
        # Output layer
        h = self.conv3(h, edge_index)
        return h.squeeze(-1)
```

#### 4.2.2 Training, Loss and Cross Validation

We train on the Root Mean Squared Percentage Error (RMSPE), defined per bucket $t$ as guided by Optiver in the Kaggle competition for the loss function (Refer Appendix for details) as it focuses on the models relative errors, unlike aboslute error which understate risk. In our version in order to prevent division by zero issues we add a small $\epsilon = 10^-8$.

```{python}
# def rmspe_loss(y_pred, y_true, eps=1e-8):
#    return torch.sqrt(torch.mean(((y_pred - y_true)/(y_true + eps))**2))

def stationary_mse_loss(y_pred, y_true):
    """
    Simple MSE loss for stationary data - much more stable than RMSPE
    """
    return F.mse_loss(y_pred, y_true)

def log_cosh_percentage_loss(y_pred, y_true):
    """
    Log-cosh loss with percentage interpretation
    Smooth, robust alternative to MSE with built-in percentage scaling
    """
    target_scale = torch.std(y_true)
    scaled_error = (y_pred - y_true) / target_scale
    log_cosh = torch.log(torch.cosh(scaled_error))
    return torch.mean(log_cosh) * 100

def relative_mse_percentage_loss(y_pred, y_true):
    """
    🏆 RECOMMENDED: MSE as percentage of target variance
    Perfect for your transformed stationary data!
    
    Returns: Percentage value (e.g., 15.5 means 15.5% error)
    """
    mse = F.mse_loss(y_pred, y_true)
    target_var = torch.var(y_true) + 1e-8  # Add small epsilon for stability
    return (mse / target_var) * 100

def hybrid_volatility_loss(y_pred, y_true, alpha=0.8):
    """
    🥉 ADVANCED: Combines magnitude + direction prediction
    Best for trading applications where direction matters
    
    Parameters:
    - alpha: Weight for magnitude error (0.8 = 80% magnitude, 20% direction)
    
    Returns: Combined percentage score
    """
    # Magnitude component (relative MSE)
    magnitude_loss = relative_mse_percentage_loss(y_pred, y_true)
    
    # Direction component (directional accuracy as loss)
    pred_direction = torch.sign(y_pred)
    true_direction = torch.sign(y_true)
    direction_accuracy = torch.mean((pred_direction == true_direction).float())
    direction_loss = (1 - direction_accuracy) * 100  # Convert to loss (lower is better)
    
    # Combine
    return alpha * magnitude_loss + (1 - alpha) * direction_loss
    
```

Despite the chronological splits advantages we wanted to make sure the model wont get a lucky/unlucky with market regime in validation period. There were unique chalenges the model faces beyond the regime shifts including Attention mechanism stability across different market conditions, Graph topology effectiveness (from KD-tree neighbors) to name a few and to elimate all these as much as possible we went with a 8 fold cross validation with early stopping to prevent overffitting on a single time period and to find the best GAT architecture (hidden dims, heads, dropout) without contaminating your final test set.

```{python}
import torch
import torch.nn.functional as F
from torch_geometric.nn import GATConv
from torch_geometric.data import Data, DataLoader
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd

def train_gat_fixed(X, y, edge_index, edge_weight, device='cpu'):
    """
    Fixed training loop with proper error handling
    """
    T, N, feat_dim = X.shape
    print(f"Training data: {T} time points, {N} nodes, {feat_dim} features")
    
    # Convert to tensors
    X_tensor = torch.tensor(X, dtype=torch.float)
    y_tensor = torch.tensor(y, dtype=torch.float)
    
    # Create graph snapshots
    graph_list = []
    for t in range(T):
        graph_list.append(Data(
            x=X_tensor[t],           # [N, feat_dim]
            edge_index=edge_index,   # [2, E]
            edge_weight=edge_weight, # [E]
            y=y_tensor[t]           # [N]
        ))
    
    print(f"Created {len(graph_list)} graph snapshots")
    
    # Time-series CV with larger validation set
    tscv = TimeSeriesSplit(n_splits=4, test_size=int(0.2 * T), gap=10)
    
    best_models = []
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(range(T))):
        print(f"\n{'='*20} Fold {fold+1}/5 {'='*20}")
        
        train_graphs = [graph_list[i] for i in train_idx]
        val_graphs = [graph_list[i] for i in val_idx]
        
        # Data loaders with larger batch size
        train_loader = DataLoader(train_graphs, batch_size=8, shuffle=True)
        val_loader = DataLoader(val_graphs, batch_size=8, shuffle=False)
        
        # Initialize model
        model = ImprovedVolatilityGAT(
            in_feats=feat_dim, 
            hidden=32,  # Smaller for stability
            heads=2,    # Fewer heads
            dropout=0.2
        ).to(device)
        
        # Optimizer with lower learning rate
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=1e-3,  # Lower learning rate
            weight_decay=1e-4
        )
        
        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.7, patience=5
        )
        
        # Training variables
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 15
        
        train_losses = []
        val_losses = []
        
        for epoch in range(1, 101):  # Max 100 epochs
            # Training
            model.train()
            train_loss = 0.0
            train_count = 0
            
            for batch in train_loader:
                batch = batch.to(device)
                optimizer.zero_grad()
                
                try:
                    preds = model(batch.x, batch.edge_index)
                    # loss = stationary_mse_loss(preds, batch.y)
                    loss = hybrid_volatility_loss(preds, batch.y)
                    
                    # Check for NaN loss
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"⚠️ NaN/Inf loss detected at epoch {epoch}")
                        break
                    
                    loss.backward()
                    
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    optimizer.step()
                    train_loss += loss.item()
                    train_count += 1
                    
                except Exception as e:
                    print(f"Training error at epoch {epoch}: {e}")
                    break
            
            if train_count == 0:
                print("Training failed - breaking")
                break
                
            train_loss /= train_count
            
            # Validation
            model.eval()
            val_loss = 0.0
            val_count = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    batch = batch.to(device)
                    try:
                        preds = model(batch.x, batch.edge_index)
                        # loss = stationary_mse_loss(preds, batch.y)
                        loss = hybrid_volatility_loss(preds, batch.y)
                        
                        if not (torch.isnan(loss) or torch.isinf(loss)):
                            val_loss += loss.item()
                            val_count += 1
                    except Exception as e:
                        print(f"Validation error: {e}")
                        continue
            
            if val_count == 0:
                print("Validation failed - breaking")
                break
                
            val_loss /= val_count
            
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            
            # Print progress
            if epoch % 5 == 0 or epoch < 10:
                print(f"Epoch {epoch:03d} | Train MSE: {train_loss:.6f} | Val MSE: {val_loss:.6f}")
            
            # Learning rate scheduling
            scheduler.step(val_loss)
            
            # Early stopping
            if val_loss < best_val_loss - 1e-6:
                best_val_loss = val_loss
                patience_counter = 0
                # Save best model
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'epoch': epoch,
                    'val_loss': val_loss,
                    'train_loss': train_loss
                }, f'best_model_fold_{fold}.pt')
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"→ Early stopping at epoch {epoch}")
                    break
        
        # Store results
        fold_results.append({
            'fold': fold,
            'best_val_loss': best_val_loss,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'final_epoch': epoch
        })
        
        print(f"Fold {fold+1} complete - Best Val MSE: {best_val_loss:.6f}")
    
    return fold_results
```



```{python}
# Step 3: Train with fixed pipeline
device = torch.device('mps' if torch.mps.is_available() else 'cpu')
print(f"Using device: {device}")

# Your edge_index and edge_weight should be already created
results = train_gat_fixed(X_transformed, y_transformed, edge_index, edge_weight, device)

# Analyze results
print("\n" + "="*50)
print("TRAINING RESULTS SUMMARY")
print("="*50)

avg_val_loss = np.mean([r['best_val_loss'] for r in results])
std_val_loss = np.std([r['best_val_loss'] for r in results])

print(f"Average validation MSE: {avg_val_loss:.6f} ± {std_val_loss:.6f}")

for i, result in enumerate(results):
    print(f"Fold {i+1}: {result['best_val_loss']:.6f} (epoch {result['final_epoch']})")

```